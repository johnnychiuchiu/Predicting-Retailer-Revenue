---
title: "Predicting Retailer Revenue"
author: Arvind Koul, Ethel Zhang, Johnny Chiu, Saurabh Tripathi, Varun Gupta, Yuqing
  Liu
date: "1/21/2018"
output:
  html_document: default
  word_document: default
---

# Outline

* Set up
* Exploratory Data Analysis
* Feature Engineer
* Model Building
* Model selection

---

### Set up
```{r, warning=FALSE, message=FALSE}
# Set working directory and source function
pwd = getwd()
source(paste(pwd,'/function.R',sep=''))

# Read Data
booktrain_path = paste(getwd(),'/../data/booktrain.csv',sep='')
booktrain = read.csv(booktrain_path)
orders_path = paste(getwd(),'/../data/orders.csv',sep='')
orders = read.csv(orders_path)

# Only keep the rows from orders that can be matched to booktrain, i.e., the rows with logtarg.
all= merge(orders, booktrain, by='id',all.x= TRUE)

# Create Train and Test Dataset
train_original = all %>% filter(!is.na(logtarg)) # note that train data contains only 8224 customer id instead of 8311, since there are 87 ids in booktrain that can not be matched to the records in orders.
train_original = data_manipulate(train_original)

test_original = all %>% filter(is.na(logtarg))
test_original = data_manipulate(test_original)
```


### Exploratory Data Analysis
```{r}
### check NA
colSums(is.na(train_original)) # do not have any NA

### check the distribution for categorical columns
ggplot(data=train_original, aes(x=category)) + geom_histogram(binwidth=1)

### check outlier for numerical columns
# > qty
# check_outlier(train_original, 'qty', 10) # I think all the outliers are correct but extreme value. Decide to keep it for now.
# check_outlier(train_original, 'price', 10) # I think all the outliers are correct but extreme value. Decide to keep it for now.

# > price
# there are 3.25% of rows with the value 0 in column 'price', and most of them are from category 99 (61%).

table(train_original[train_original$price==0,]$category)/dim(train_original[train_original$price==0,])[1]
```



### Feature Engineering
```{r, warning=FALSE, message=FALSE, cache=TRUE}
train = feature_engineer(train_original)
test = feature_engineer(test_original, isTrain=FALSE)
```

Here are the list 

| Feature | Description |
|---------------------------------------------------- | ----------------------------------------------------|
| days_recent_purchase | days since last purchase  |
| days_first_purchase | days since first purchase |
| order_count | total number of orders |
| avg_qty | average quantity per order|
| avg_ord_value | average order price |
| slope | build a linear regression for each id using date as predictor and daily total quantity as response and use the slope as the trend |
| coe_va | Coefficient of variation using the qty over year. The value is calculated using the variance divide by mean|
| cat_count | number of categories purcahsed per user|
| entrophy| how diverse the amount of purchase are from all categories. The value is calculated using entropy function|
| total_money| total money spent per user|
| avg_cats| Average number of categories per order|
| distinct_orddate| count distinct order date per user|
| aug_after_orders| ??|
| ordperyr| order per year|

### Model Building

**Fit a simple regression model for benchmark**

```{r}
fit_multiple = lm(logtarg~.-id, data = train)
summary(fit_multiple) # Residual standard error: 0.6114 | Adjusted R-squared:  0.01354
```

**Stepwise Linear Regression**

```{r, warning=FALSE, message=FALSE}
# **Backward**
fit_stepback <- stepAIC(fit_multiple,direction = c("backward"),trace=FALSE)

# **Forward**
fit_zero <- lm(logtarg ~ 1, data = train)
fit_stepforw <- stepAIC(fit_zero,direction = c("forward"), scope=list(upper=fit_multiple,lower=fit_zero),trace=FALSE)
```

```{r}
summary(fit_stepback) # Residual standard error: 0.6112 | Adjusted R-squared:  0.01422
summary(fit_stepforw) # Residual standard error: 0.6112 | Adjusted R-squared:  0.01418
```

**Lasso Linear Regression**

```{r}
y=train$logtarg
x=model.matrix(logtarg~.-id,train)
param_lasso = my_cv_glmnet(y,x,1)$small.lambda.betas
param_lasso = param_lasso[param_lasso!=0]
lasso_feature = c('logtarg','days_recent_purchase', 'days_first_purchase', 'order_count', 'avg_qty','coe_va')
fit_lasso = lm(logtarg~., data = train[lasso_feature])
summary(fit_lasso) # Residual standard error: 0.6113 | Adjusted R-squared:  0.01386
```


### Model selection
Choose the model according to Root Mean Square Error(RMSE) and Adjusted R-squared

```{r, echo=FALSE, results='asis'}
table_result <- data.frame(
  Model=c('Multiple Linear Regression','Stepwise Regression Backward', 'Stepwise Regression Forward', 'Lasso Regression'),
  RMSE = c(summary(fit_multiple)$sigma, summary(fit_stepback)$sigma, summary(fit_stepforw)$sigma, summary(fit_lasso)$sigma),
  adj_R_square = c(summary(fit_multiple)$adj.r.squared, summary(fit_stepback)$adj.r.squared, summary(fit_stepforw)$adj.r.squared, summary(fit_lasso)$adj.r.squared))

kable(table_result %>% arrange(RMSE, desc(adj_R_square)))
```


In our final model, the significant features are 


| Feature | Description |
|---------------------------------------------------- | ----------------------------------------------------|
| days_first_purchase | days since first purchase |
| order_count | total number of orders |
| avg_qty | average quantity per order|
| slope | build a linear regression for each id using date as predictor and daily total quantity as response and use the slope as the trend |
| coe_va | Coefficient of variation using the qty over year. The value is calculated using the variance divide by mean|
| cat_count | number of categories purcahsed per user|
| total_money| total money spent per user|
| aug_after_orders| ??|


### Make prediction
```{r}
predict_reg = predict(fit_multiple, newdata=test)
```

### Generating submitting file
```{r}
test$logtarg = predict_reg
file_version1 = test %>% dplyr::select(id, logtarg)
file_version1$logtarg = ifelse(file_version1$logtarg<0, 0, file_version1$logtarg)
colnames(file_version1) = c('id','yhat')
# write.csv(file_version1, file=paste('./submission/model_',format(Sys.time(), "%b%d_%H%M%S"),'.csv',sep=''),row.names = FALSE)
```
